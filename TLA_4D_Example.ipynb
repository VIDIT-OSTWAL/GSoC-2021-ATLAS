{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "TLA_4D_Example.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev-iwqlJi4fE"
      },
      "source": [
        "In this notebook, we will run a ready-made network starting from some ATLAS data, which is already normalized. There is also an alternative to train the network from scratch.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2U3W3adi4fM"
      },
      "source": [
        "## Look into the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNOaCTvpi4fQ"
      },
      "source": [
        "First we need to make sure that Python 3.8 is used in the notebook. It is required in order to open this certain .pkl-file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBeFd-uutsJn"
      },
      "source": [
        "#importing various libraries\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4sbVLoZvC8b"
      },
      "source": [
        "#we need to find the maximum number of particle in one single event\r\n",
        "#this is done because we have different number of columns for each row, and we need to specify number of columns while \r\n",
        "# using pd.read_csv()\r\n",
        "import sys\r\n",
        "no_of_column = -1\r\n",
        "#no_of_column is initially negative as no dataset will have a negative number of columns \r\n",
        "index11 = 1\r\n",
        "# index11 is used to keep a track of how many iteration have been done\r\n",
        "while (no_of_column == -1):\r\n",
        "  \r\n",
        "  try :\r\n",
        "    my_cols = [str(i) for i in range(index11)]\r\n",
        "    reading_data = pd.read_csv(\"/content/monojet_Zp2000.0_DM_50.0_chan3.csv\",sep =',|;',names = my_cols, header=None)\r\n",
        "    no_of_column  =  index11\r\n",
        "  except :\r\n",
        "    index11 = index11 + 1\r\n",
        "\r\n",
        "print(no_of_column)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RhcwpAReJdb"
      },
      "source": [
        "#two different delimiter are used (, and ;)\r\n",
        "#as the number of columns are not same in every row, therefore we need to specify number of columns while using pd.read_csv()\r\n",
        "#Note : the exact number of columns required was obtained by iteration method and hit and trail method \r\n",
        "\r\n",
        "my_cols = [str(i) for i in range(no_of_column)]\r\n",
        "train_data1 = pd.read_csv(\"/content/monojet_Zp2000.0_DM_50.0_chan3.csv\",sep =',|;',names = my_cols, header=None, engine = 'python')\r\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaQVjS2KgxdH"
      },
      "source": [
        "#initial columns of dataset, [\"event ID\",\"process ID\",\"event weight\",\"MET\",\"METphi\"] contain information about the event, and hence not useful \r\n",
        "#as the next 'type' object is separated from the previous particle 'phi' detail with a ';' delimiter, all last columns will have \r\n",
        "#'NaN' value instead of 'type' detail, therefore all these columns are dropped.\r\n",
        "train_data1.drop(['0','1','2','3','4'],axis = 1 ,inplace = True)\r\n",
        "train_data1.drop([str(len(my_cols) -1 )],axis = 1 ,inplace = True)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuBDB3kWipYW"
      },
      "source": [
        "# as each particle will have 5 reading,['type','E','pt','eta','phi'] \r\n",
        "max_reading = int(len(train_data1.columns)/5)\r\n",
        "# max_reading tells maximum number of particle information which a row compared to all the rows will have."
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkFHOpkIipcH"
      },
      "source": [
        "#check about how data looks right now\r\n",
        "train_data1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa2hciGsipfa"
      },
      "source": [
        "# 1st particle of each row, first 5 columns are sliced into a new dataframe data1, columns names are given, and all the na values rows\r\n",
        "# are droppped, incase if in a event no particle information is recorded. \r\n",
        "data1 = train_data1[train_data1.columns[0:5]]\r\n",
        "data1.columns = ['Type', 'm', 'pt', 'eta', 'phi']\r\n",
        "data1.dropna(inplace = True)\r\n",
        "print(data1)\r\n",
        "#data1 right now is of shape (6399,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zANImKbLipib"
      },
      "source": [
        "# the same slicing is done for all the columns, stored into a local dataframe 'data', the na values are removed and dataframe is stored in \r\n",
        "# 'remove' and then data and data1 dataframes are concated\r\n",
        "# Note: 'm' name column represent data of 'E'\r\n",
        "for index in range(1, max_reading):\r\n",
        "  data = train_data1[train_data1.columns[ 5 * index : 5*(index + 1)]]\r\n",
        "  data.columns = ['Type', 'm', 'pt', 'eta', 'phi']\r\n",
        "  remove = data.dropna()\r\n",
        "  data1 = pd.concat([data1, remove], axis=0)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mhQpz7-mbQK"
      },
      "source": [
        "data1.shape\r\n",
        "# data1, concated with all particle information in 5 columns now has total of 23473 rows and 5 columns with no na values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzlzH8WGnCn7"
      },
      "source": [
        "# all rows with particle of type 'j' is selected and stored in data_select\r\n",
        "data_select = data1.loc[data1['Type'] == 'j']\r\n",
        "data_select = data_select[['pt', 'eta', 'phi', 'm']]\r\n",
        "# the columns are rearranged in ['pt','eta','phi','m'] format"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G7eizlHoNQJ"
      },
      "source": [
        "# the dataset is broken randomly into 80:20 ratio with help of train_test_split\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "train, test = train_test_split(data_select, test_size=0.2,random_state = 45)\r\n",
        "\r\n",
        "print(train.shape)\r\n",
        "print(test.shape)\r\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-JIRe97ofMV"
      },
      "source": [
        "#feature scaling is done, as the data is very screwed\r\n",
        "#feature scaling is done after dividing the dataset into train and test, to avoid any bias that one set can do on other one .\r\n",
        "#Standard Scaler is used for feature MinMaxScaler\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "train[['pt', 'eta', 'phi','m']] = MinMaxScaler().fit_transform(train[['pt', 'eta', 'phi','m']])\r\n",
        "test[['pt', 'eta', 'phi','m']] = MinMaxScaler().fit_transform(test[['pt', 'eta', 'phi','m']])\r\n",
        "\r\n",
        "\r\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf2Ifcwsp_FZ"
      },
      "source": [
        "print(train)\r\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8ST35oEqPqe"
      },
      "source": [
        "# plot to know what type of data we have after feature scaling\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "unit_list = ['[log(GeV)]', '[rad/3]', '[rad/3]', '[log(GeV)]']\r\n",
        "variable_list = [r'$p_T$', r'$\\eta$', r'$\\phi$', r'$m$']\r\n",
        "\r\n",
        "branches=['pt', 'eta', 'phi', 'm']\r\n",
        "\r\n",
        "n_bins = 100\r\n",
        "\r\n",
        "for kk in range(0,4):\r\n",
        "    n_hist_data, bin_edges, _ = plt.hist(train[branches[kk]], color='blue', label='Input', alpha=1, bins=n_bins)\r\n",
        "    plt.xlabel(xlabel=variable_list[kk] + ' ' + unit_list[kk])\r\n",
        "    plt.ylabel('# of events')\r\n",
        "    plt.savefig(\"fourmomentum_\"+branches[kk],dpi=300)\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqB2dbp9k6RZ"
      },
      "source": [
        "# there was a problem running this file on local python environment, this notebook was runned on colab\r\n",
        "\r\n",
        "!pip install -Uqq fastbook\r\n",
        "import fastbook\r\n",
        "fastbook.setup_book()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwctE9Nhi4fX"
      },
      "source": [
        "We take a pickle dataset, and open into Pandas (after importing pandas). Note that you have to change the paths to the directory where your processed files are. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mNpuuaEi4fb"
      },
      "source": [
        "Now we plot the data using the matplotlib library. The units reflect the normalization, but it's the shape that we care about. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bhFWHzIi4fg"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihfGH3dqi4fh"
      },
      "source": [
        "Adding the two datasets as TensorDatasets to PyTorch (also loading all other classes we'll need later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DumFqYz2i4fi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from fastai import learner\n",
        "from fastai.data import core\n",
        "\n",
        "train_x = train\n",
        "test_x = test\n",
        "train_y = train_x  # y = x since we are building an autoencoder\n",
        "test_y = test_x\n",
        "\n",
        "# Constructs a tensor object of the data and wraps them in a TensorDataset object.\n",
        "train_ds = TensorDataset(torch.tensor(train_x.values, dtype=torch.float), torch.tensor(train_y.values, dtype=torch.float))\n",
        "valid_ds = TensorDataset(torch.tensor(test_x.values, dtype=torch.float), torch.tensor(test_y.values, dtype=torch.float))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHGD7OsTi4fk"
      },
      "source": [
        "We now set things up to load the data, and we use a batch size that was optimized by previous students...note also that this is fastai v2, migration thanks to Jessica Lastow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVeJRkVci4fl"
      },
      "source": [
        "bs = 256\n",
        "\n",
        "# Converts the TensorDataset into a DataLoader object and combines into one DataLoaders object (a basic wrapper\n",
        "# around several DataLoader objects). \n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
        "dls = core.DataLoaders(train_dl, valid_dl)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtB_ZtHwi4fm"
      },
      "source": [
        "### Preparing the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXwbTSGwi4fn"
      },
      "source": [
        "Here we have an example network. Details aren't too important, as long as they match what was already trained for us...in this case we have a LeakyReLU, tanh activation function, and a number of layers that goes from 4 to 200 to 20 to 3 (number of features in the hidden layer that we pick for testing compression) and then back all the way to 4. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1sorPmEi4fo"
      },
      "source": [
        "class AE_3D_200_LeakyReLU(nn.Module):\n",
        "    def __init__(self, n_features=4):\n",
        "        super(AE_3D_200_LeakyReLU, self).__init__()\n",
        "        self.en1 = nn.Linear(n_features, 200)\n",
        "        self.en2 = nn.Linear(200, 200)\n",
        "        self.en3 = nn.Linear(200, 20)\n",
        "        self.en4 = nn.Linear(20, 3)\n",
        "        self.de1 = nn.Linear(3, 20)\n",
        "        self.de2 = nn.Linear(20, 200)\n",
        "        self.de3 = nn.Linear(200, 200)\n",
        "        self.de4 = nn.Linear(200, n_features)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.en4(self.tanh(self.en3(self.tanh(self.en2(self.tanh(self.en1(x)))))))\n",
        "\n",
        "    def decode(self, x):\n",
        "        return self.de4(self.tanh(self.de3(self.tanh(self.de2(self.tanh(self.de1(self.tanh(x))))))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        return self.decode(z)\n",
        "\n",
        "    def describe(self):\n",
        "        return 'in-200-200-20-3-20-200-200-out'\n",
        "\n",
        "#model = AE_3D_200_LeakyReLU().double()\n",
        "model = AE_3D_200_LeakyReLU()\n",
        "model.to('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_09iSFyMi4fq"
      },
      "source": [
        "We now have to pick a loss function - MSE loss is appropriate for a compression autoencoder since it reflects the [(input-output)/input] physical quantity that we want to minimize. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFkTyr1Yi4fr"
      },
      "source": [
        "from fastai.metrics import mse\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "#bn_wd = False  # Don't use weight decay for batchnorm layers\n",
        "#true_wd = True  # weight decay will be used for all optimizers\n",
        "wd = 1e-6\n",
        "\n",
        "recorder = learner.Recorder()\n",
        "learn = learner.Learner(dls, model=model, wd=wd, loss_func=loss_func, cbs=recorder)\n",
        "#was: learn = basic_train.Learner(data=db, model=model, loss_func=loss_func, wd=wd, callback_fns=ActivationStats, bn_wd=bn_wd, true_wd=true_wd)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61wQ9TCJi4fs"
      },
      "source": [
        "## Alternative 1: Running a pre-trained network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiHj-JbKi4ft"
      },
      "source": [
        "Now we load the pre-trained network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vskm8DuLi4f0"
      },
      "source": [
        "## Alternative 2: Training a new network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6YA9dvni4f1"
      },
      "source": [
        "Instead of using a pre-trained network, an alternative is to train a new network and use that instead. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK-BI6Rti4f2"
      },
      "source": [
        "First, we want to find the best learning rate. The learning rate is a hyper-paramater that sets how much the weights of the network will change each step with respect to the loss gradient.\n",
        "\n",
        "Then we plot the loss versus the learning rates. We're interested in finding a good order of magnitude of learning rate, so we plot with a log scale.\n",
        "\n",
        "A good value for the learning rates is then either:\n",
        "- one tenth of the minimum before the divergence\n",
        "- when the slope is the steepest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzHYE78li4f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "3f0c9313-e309-483b-8b47-ff69b35381fb"
      },
      "source": [
        "from fastai.callback import schedule\n",
        "\n",
        "lr_min, lr_steep = learn.lr_find()\n",
        "\n",
        "print('Learning rate with the minimum loss:', lr_min)\n",
        "print('Learning rate with the steepest gradient:', lr_steep)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate with the minimum loss: 0.0019054606556892395\n",
            "Learning rate with the steepest gradient: 0.0003311311302240938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1HLsj68i4f4"
      },
      "source": [
        "Now we want to run the training!\n",
        "\n",
        "User-chosen variables:\n",
        "- n_epoch: The number of epochs, i.e how many times the to run through all of the training data once (i.e the 1266046 entries, see cell 2)\n",
        "- lr: The learning rate. Either choose lr_min, lr_steep from above or set your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_YwI_Byi4f5"
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.perf_counter() # Starts timer\n",
        "learn.fit_one_cycle(n_epoch=100, lr_max=lr_min)\n",
        "end = time.perf_counter() # Ends timer\n",
        "delta_t = end - start\n",
        "print('Training took', delta_t, 'seconds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFysfl7vi4f7"
      },
      "source": [
        "Then we plot the loss as a function of batches and epochs to check if we reach a plateau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S8JArDUi4f7"
      },
      "source": [
        "recorder.plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9Bw73aKi4f9"
      },
      "source": [
        "Then we evaluate the MSE on this network - it should be of the order of 0.001 or less if all has gone well...if it has not trained as well (note the pesky 0-mass peak above...) then it's going to be a bit higher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N-icBJai4f-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c85820d2-055f-46ea-dd84-37d87f42a423"
      },
      "source": [
        "learn.validate()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#1) [6.476144335465506e-05]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yCEB67di4f_"
      },
      "source": [
        "Let's plot all of this, with ratios (thanks to code by Erik Wallin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwgo0MBti4gB"
      },
      "source": [
        "## Plotting the outputs of the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISInsn3Gi4gC"
      },
      "source": [
        "Lazy-save of our output files (they'll also be on screen)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fEnW5kii4gC"
      },
      "source": [
        "import os\n",
        "save_dir = \"plotOutput\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_OBDOTui4gD"
      },
      "source": [
        "A function in case we want to un-normalize and get back to physical quantities..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtn1VryBi4gE"
      },
      "source": [
        "def custom_unnormalize(df):\n",
        "    df['eta'] = df['eta'] * 5\n",
        "    df['phi'] = df['phi'] * 3\n",
        "    #df['E'] = 10**df['E']\n",
        "    df['m'] = 10**df['m']\n",
        "    df['pt'] = 10**(df['pt'])\n",
        "    return df"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HfjxiK_i4gF"
      },
      "source": [
        "Make the histograms from the dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxJGj-dBi4gF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "plt.close('all')\n",
        "unit_list = ['[GeV]', '[rad]', '[rad]', '[GeV]']\n",
        "variable_list = [r'$p_T$', r'$\\eta$', r'$\\phi$', r'$E$']\n",
        "line_style = ['--', '-']\n",
        "colors = ['orange', 'c']\n",
        "markers = ['*', 's']\n",
        "\n",
        "model.to('cpu')\n",
        "\n",
        "save = True # Option to save figure\n",
        "\n",
        "# Histograms\n",
        "idxs = (0, 100000)  # Choose events to compare\n",
        "data = torch.tensor(test[idxs[0]:idxs[1]].values, dtype=torch.float)\n",
        "#data = torch.tensor(test[idxs[0]:idxs[1]].values, dtype=torch.float).double()\n",
        "pred = model(data)\n",
        "pred = pred.detach().numpy()\n",
        "data = data.detach().numpy()\n",
        "\n",
        "data_df = pd.DataFrame(data, columns=test.columns)\n",
        "pred_df = pd.DataFrame(pred, columns=test.columns)\n",
        "\n",
        "unnormalized_data_df = custom_unnormalize(data_df)\n",
        "unnormalized_pred_df = custom_unnormalize(pred_df)    \n",
        "    \n",
        "alph = 0.8\n",
        "n_bins = 200\n",
        "for kk in np.arange(4):\n",
        "    plt.figure()\n",
        "    n_hist_data, bin_edges, _ = plt.hist(data[:, kk], color=colors[1], label='Input', alpha=1, bins=n_bins)\n",
        "    n_hist_pred, _, _ = plt.hist(pred[:, kk], color=colors[0], label='Output', alpha=alph, bins=bin_edges)\n",
        "    plt.suptitle(test.columns[kk])\n",
        "    plt.xlabel(test.columns[kk])\n",
        "    plt.ylabel('Number of events')\n",
        "    # ms.sciy()\n",
        "    plt.yscale('log')\n",
        "    if save:\n",
        "        plt.savefig(os.path.join(save_dir,test.columns[kk]+'.png'))\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE14KV4Ti4gH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27765e03-33e8-4e69-9ae6-fcb887b80424"
      },
      "source": [
        "def getRatio(bin1,bin2):\n",
        "    bins = []\n",
        "    for b1,b2 in zip(bin1,bin2):\n",
        "        if b1==0 and b2==0:\n",
        "            bins.append(0.)\n",
        "        elif b2==0:\n",
        "            bins.append(None)\n",
        "        else:\n",
        "            bins.append((float(b2)-float(b1))/b1)\n",
        "    return bins   \n",
        "\n",
        "rat = getRatio(n_hist_data,n_hist_pred)\n",
        "print(rat)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.06771463119709795, 0.03166666666666667, -0.017977528089887642, -0.008547008547008548, -0.008368200836820083, 0.019138755980861243, 0.012658227848101266, -0.06875, 0.21875, -0.2108843537414966, 0.3870967741935484, -0.15789473684210525, 0.13043478260869565, -0.12359550561797752, 0.06493506493506493, -0.06172839506172839, 0.01818181818181818, 0.01639344262295082, -0.08771929824561403, -0.24, 0.022727272727272728, -0.14634146341463414, -0.24324324324324326, -0.03125, -0.41379310344827586, 0.0, 0.23529411764705882, -0.05263157894736842, -0.047619047619047616, -0.42105263157894735, 0.36363636363636365, -0.42105263157894735, 0.3076923076923077, 0.7142857142857143, -0.5, 0.0, -0.6, -0.5555555555555556, 1.0, -0.16666666666666666, -0.5, 0.25, -0.3333333333333333, -0.2, -0.6, 2.0, -0.25, 1.3333333333333333, 0.25, 1.0, -0.4, None, -0.8333333333333334, -0.6666666666666666, 1.0, None, 3.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, -0.75, inf, 0.0, 0.0, None, None, 0.0, inf, 1.0, None, inf, -0.5, inf, 0.0, 0.0, None, -0.6666666666666666, inf, 0.0, None, 0.0, None, 0.0, inf, inf, 0.0, inf, 0.0, None, None, 1.0, inf, 0.0, 0.0, None, 1.0, 0.0, 0.0, 0.0, None, 0.0, 0.0, inf, None, inf, 0.0, 0.0, 0.0, 0.0, 0.0, None, None, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, inf, 0.0, 0.0, inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, inf, None, 0.0, 0.0, 0.0, inf, inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, None, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, None, 0.0, None, 0.0, 0.0, 0.0, inf, None, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, None, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, None]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPwWdCl2i4gJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
